{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of German Bundestag Speeches: Comparing Lexicon-based and Model-based Approaches\n",
    "\n",
    "In this notebook, we analyze speeches from the German Bundestag and compare two fundamentally different approaches to text analysis:\n",
    "\n",
    "1. **Lexicon-based approaches**: Work with predefined dictionaries and rules\n",
    "2. **Model-based approaches**: Use machine learning and pre-trained AI models\n",
    "\n",
    "We will apply both approaches to two tasks:\n",
    "- **Named Entity Recognition (NER)**: Finding location names in texts\n",
    "- **Sentiment Analysis**: Determining the emotional tone (positive/negative/neutral)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Understand the differences between rule-based and ML-based approaches\n",
    "- Apply both methods practically\n",
    "- Assess the advantages and disadvantages of both approaches\n",
    "- Compare and interpret results\n",
    "\n",
    "## Structure\n",
    "\n",
    "1. Setup and Data Loading\n",
    "2. **Lexicon-based Approaches**\n",
    "   - Named Entity Recognition with location dictionary\n",
    "   - Sentiment Analysis with SentiWS lexicon\n",
    "3. **Model-based Approaches**\n",
    "   - Named Entity Recognition with spaCy\n",
    "   - Sentiment Analysis with Transformers\n",
    "4. Comparison and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Installation\n",
    "\n",
    "First, we will install all required packages. You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (only needed the first time)\n",
    "!pip install spacy transformers pandas torch tqdm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We load the already extracted Bundestag speeches from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv('reden.csv')\n",
    "\n",
    "print(f\"Number of speeches: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brief overview of the data\n",
    "print(\"Distribution by faction:\")\n",
    "print(df['fraktion'].value_counts())\n",
    "\n",
    "print(f\"\\nAverage length of a speech: {df['fliesstext'].str.len().mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Lexicon-based Approaches\n",
    "\n",
    "In this section, we use **predefined dictionaries and word lists** to analyze texts. These approaches are:\n",
    "- ✓ **Transparent**: Every decision is traceable\n",
    "- ✓ **Fast**: Very quick processing\n",
    "- ✓ **Controllable**: You can modify the lexicons\n",
    "- ✗ **Limited**: Only finds what's in the dictionary\n",
    "- ✗ **No context understanding**: Cannot understand negations or irony\n",
    "\n",
    "We will implement two lexicon-based approaches:\n",
    "1. **NER**: Finding place names using a location dictionary\n",
    "2. **Sentiment Analysis**: Analyzing emotional tone using the SentiWS lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lexicon-based NER: Finding Place Names\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. We create a list of German locations (cities, states, regions)\n",
    "2. We split each text into words\n",
    "3. For each word, we check: Is it in our location dictionary?\n",
    "4. If yes, we mark it as a location\n",
    "\n",
    "This is the **simplest form of Named Entity Recognition**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the location lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base lexicon: German cities and federal states\n",
    "# ADJUSTMENT: You can add or remove locations here! (Erfurt, Thüringen)\n",
    "\n",
    "german_locations = {\n",
    "    # Major cities\n",
    "    'Berlin', 'Hamburg', 'München', 'Köln', 'Frankfurt',\n",
    "    'Stuttgart', 'Düsseldorf', 'Dortmund', 'Essen', 'Leipzig',\n",
    "    'Bremen', 'Dresden', 'Hannover', 'Nürnberg', 'Duisburg',\n",
    "    'Bochum', 'Wuppertal', 'Bielefeld', 'Bonn', 'Münster',\n",
    "    'Karlsruhe', 'Mannheim', 'Augsburg', 'Wiesbaden', 'Gelsenkirchen',\n",
    "    'Mönchengladbach', 'Braunschweig', 'Chemnitz', 'Kiel', 'Aachen',\n",
    "    'Halle', 'Magdeburg', 'Freiburg', 'Krefeld', 'Lübeck',\n",
    "    'Oberhausen', 'Mainz', 'Rostock', 'Kassel',\n",
    "    \n",
    "    # Federal states\n",
    "    'Baden-Württemberg', 'Bayern', 'Brandenburg', 'Hessen',\n",
    "    'Mecklenburg-Vorpommern', 'Niedersachsen', 'Nordrhein-Westfalen',\n",
    "    'Rheinland-Pfalz', 'Saarland', 'Sachsen', 'Sachsen-Anhalt',\n",
    "    'Schleswig-Holstein',\n",
    "    \n",
    "    # Regions\n",
    "    'Ruhrgebiet', 'Rheinland', 'Franken', 'Schwaben', 'Pfalz',\n",
    "    'Ostfriesland', 'Allgäu', 'Schwarzwald'\n",
    "}\n",
    "\n",
    "# For searching: both original and lowercase\n",
    "locations_lower = {loc.lower(): loc for loc in german_locations}\n",
    "\n",
    "print(f\"Location lexicon created with {len(german_locations)} entries\")\n",
    "print(f\"\\nExamples: {list(german_locations)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement the search function\n",
    "\n",
    "Now we program the function that searches for locations in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_locations_lexicon(text, location_dict):\n",
    "    \"\"\"\n",
    "    Finds locations in a text based on a lexicon.\n",
    "    \n",
    "    How it works (very simple):\n",
    "    1. Split the text into individual words (at spaces)\n",
    "    2. For each word: Check if it's in our location lexicon\n",
    "    3. If yes: Save it\n",
    "    \n",
    "    This is the simplest form of Named Entity Recognition!\n",
    "    \n",
    "    Parameters:\n",
    "        text: The text to analyze\n",
    "        location_dict: Dictionary with locations (lowercase -> Original)\n",
    "    \n",
    "    Return:\n",
    "        List of found locations\n",
    "    \"\"\"\n",
    "    found_locations = []\n",
    "    \n",
    "    # Split text into words (at spaces)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Go through each word\n",
    "    for word in words:\n",
    "        # Remove punctuation (e.g., \"Berlin,\" -> \"Berlin\")\n",
    "        # This is important because \"Berlin,\" is not in our lexicon\n",
    "        word_clean = word.strip('.,;:!?()[]\"\\'')\n",
    "        \n",
    "        # Check if the word (in lowercase) is in our lexicon\n",
    "        # We use .lower() because our lexicon is lowercase\n",
    "        if word_clean.lower() in location_dict:\n",
    "            # Add the original spelling (e.g., \"Berlin\" instead of \"berlin\")\n",
    "            found_locations.append(location_dict[word_clean.lower()])\n",
    "    \n",
    "    return found_locations\n",
    "\n",
    "# Test with example texts\n",
    "test_texts = [\n",
    "    \"In Berlin und München wird viel über die Politik in Bayern und Sachsen diskutiert.\",\n",
    "    \"Hamburg, Bremen und Kiel sind wichtige Hafenstädte.\",\n",
    "    \"Die Bundesregierung tagt in Berlin, die thüringische Landesregierung in Erfurt.\"\n",
    "]\n",
    "\n",
    "print(\"Function tests:\\n\")\n",
    "for i, test_text in enumerate(test_texts, 1):\n",
    "    test_result = find_locations_lexicon(test_text, locations_lower)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"  Text: {test_text}\")\n",
    "    print(f\"  Found locations: {test_result}\")\n",
    "    print(f\"  Count: {len(test_result)}\\n\")\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"The function only finds locations that\")\n",
    "print(\"  1. Are in the lexicon AND\")\n",
    "print(\"  2. Appear exactly like that in the text\")\n",
    "print(\"\\nExperiment: Add a new location to the lexicon above\")\n",
    "print(\"   and run the cell again. Will it be found now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The function only finds locations that\n",
    "1. Are in the lexicon AND\n",
    "2. Appear exactly like that in the text\n",
    "\n",
    "Add 'Erfurt' to the lexicon in the cell above and run both cells again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply to all speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to all speeches\n",
    "print(\"Analyzing speeches with lexicon-based NER...\")\n",
    "\n",
    "df['ner_lexicon'] = df['fliesstext'].apply(\n",
    "    lambda text: find_locations_lexicon(text, locations_lower)\n",
    ")\n",
    "\n",
    "# Number of found locations per speech\n",
    "df['ner_lexicon_count'] = df['ner_lexicon'].apply(len)\n",
    "\n",
    "print(f\"✓ Analysis completed!\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"- Speeches with at least one location: {(df['ner_lexicon_count'] > 0).sum()} of {len(df)}\")\n",
    "print(f\"- Average {df['ner_lexicon_count'].mean():.2f} locations per speech\")\n",
    "print(f\"- Maximum: {df['ner_lexicon_count'].max()} locations in one speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which locations are mentioned most frequently?\n",
    "all_locations_lexicon = [loc for locs in df['ner_lexicon'] for loc in locs]\n",
    "location_counts_lexicon = Counter(all_locations_lexicon)\n",
    "\n",
    "print(\"Top 15 most mentioned locations (lexicon method):\")\n",
    "for location, count in location_counts_lexicon.most_common(15):\n",
    "    print(f\"  {location}: {count}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Lexicon-based NER\n",
    "\n",
    "**What did we observe?**\n",
    "- The method only finds locations that are in our lexicon\n",
    "- The results are 100% traceable\n",
    "- Small towns or districts are not found (unless we add them)\n",
    "\n",
    "**Advantages:**\n",
    "- Very transparent and understandable\n",
    "- Fast processing\n",
    "- No false positives (if the lexicon is correct)\n",
    "- Easy to customize\n",
    "\n",
    "**Disadvantages:**\n",
    "- Only finds known locations\n",
    "- Cannot handle variations (e.g., \"Münchner\" for \"München\")\n",
    "- Requires manual maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Lexicon-based Sentiment Analysis with SentiWS\n",
    "\n",
    "### How it works:\n",
    "\n",
    "**SentiWS** is a German sentiment lexicon from the University of Leipzig. It contains approximately 1,800 positive and 1,800 negative words with weights.\n",
    "\n",
    "**The algorithm:**\n",
    "1. Split text into words\n",
    "2. For each word: Look up the score in the lexicon\n",
    "   - Positive words have scores > 0 (e.g., \"gut\" = +0.5)\n",
    "   - Negative words have scores < 0 (e.g., \"schlecht\" = -0.5)\n",
    "   - Neutral words are not in the lexicon (score = 0)\n",
    "3. Calculate the average of all scores\n",
    "4. Classify based on a threshold:\n",
    "   - Score > threshold → positive\n",
    "   - Score < -threshold → negative\n",
    "   - Otherwise → neutral\n",
    "\n",
    "### Why is this interesting?\n",
    "\n",
    "- Do factions differ in their emotional language?\n",
    "- Are opposition speeches more negative than government speeches?\n",
    "- How objective or emotional are political debates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load SentiWS lexicon\n",
    "\n",
    "**Important**: Make sure the SentiWS files are in the same folder!\n",
    "\n",
    "(The files were originally downloaded from: https://wortschatz.uni-leipzig.de/de/download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiws(positive_file, negative_file):\n",
    "    \"\"\"\n",
    "    Loads the SentiWS lexicon.\n",
    "    \n",
    "    File format:\n",
    "    Word|POS    Score    Inflections\n",
    "    e.g.: Abbau|NN    -0.058    Abbaus,Abbaues,Abbaue,Abbauten\n",
    "    \n",
    "    We extract:\n",
    "    - The base word (\"Abbau\")\n",
    "    - The score (-0.058)\n",
    "    - All inflections (declined forms)\n",
    "    \n",
    "    Return:\n",
    "        Dictionary: {word: score}\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    \n",
    "    # Load positive words\n",
    "    try:\n",
    "        with open(positive_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    # Extract base word (before the |)\n",
    "                    word = parts[0].split('|')[0]\n",
    "                    score = float(parts[1])\n",
    "                    lexicon[word.lower()] = score\n",
    "                    \n",
    "                    # Add inflections (if present)\n",
    "                    if len(parts) >= 3 and parts[2]:\n",
    "                        inflections = parts[2].split(',')\n",
    "                        for infl in inflections:\n",
    "                            lexicon[infl.lower()] = score\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {positive_file}\")\n",
    "        print(\"Please download SentiWS from:\")\n",
    "        print(\"https://wortschatz.uni-leipzig.de/de/download\")\n",
    "        return None\n",
    "    \n",
    "    # Load negative words\n",
    "    try:\n",
    "        with open(negative_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    word = parts[0].split('|')[0]\n",
    "                    score = float(parts[1])\n",
    "                    lexicon[word.lower()] = score\n",
    "                    \n",
    "                    if len(parts) >= 3 and parts[2]:\n",
    "                        inflections = parts[2].split(',')\n",
    "                        for infl in inflections:\n",
    "                            lexicon[infl.lower()] = score\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {negative_file}\")\n",
    "        return None\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "# Load SentiWS\n",
    "# Make sure the files are in the same folder!\n",
    "sentiws = load_sentiws(\n",
    "    'SentiWS_v2.0_Positive.txt',\n",
    "    'SentiWS_v2.0_Negative.txt'\n",
    ")\n",
    "\n",
    "if sentiws:\n",
    "    print(f\"✓ SentiWS successfully loaded!\")\n",
    "    print(f\"Lexicon contains {len(sentiws)} words\\n\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"Examples of positive words:\")\n",
    "    # Filter for positive words first, then take examples\n",
    "    positive_words = {k: v for k, v in sentiws.items() if v > 0}\n",
    "    positive_items = list(positive_words.items())\n",
    "    random.shuffle(positive_items)\n",
    "    for word, score in positive_items[:5]:\n",
    "        print(f\"  {word}: {score:+.3f}\")\n",
    "    \n",
    "    print(\"\\nExamples of negative words:\")\n",
    "    # Filter for negative words first, then take examples\n",
    "    negative_words = {k: v for k, v in sentiws.items() if v < 0}\n",
    "    negative_items = list(negative_words.items())\n",
    "    random.shuffle(negative_items)\n",
    "    for word, score in negative_items[:5]:\n",
    "        print(f\"  {word}: {score:+.3f}\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Positive words: {len(positive_words)}\")\n",
    "    print(f\"  Negative words: {len(negative_words)}\")\n",
    "    print(f\"  Total: {len(sentiws)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement sentiment analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_lexicon(text, lexicon):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of a text using a lexicon.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Split text into words\n",
    "    2. For each word: Look up the score in the lexicon\n",
    "    3. Calculate average of all scores\n",
    "    \n",
    "    Parameters:\n",
    "        text: Text to analyze\n",
    "        lexicon: SentiWS dictionary\n",
    "    \n",
    "    Return:\n",
    "        (score, details)\n",
    "    \"\"\"\n",
    "    # Split text into words (only letters)\n",
    "    words = re.findall(r'\\b[a-zäöüß]+\\b', text.lower())\n",
    "    \n",
    "    # Collect scores\n",
    "    scores = []\n",
    "    sentiment_words = []  # Which words contribute to sentiment?\n",
    "    \n",
    "    for word in words:\n",
    "        if word in lexicon:\n",
    "            score = lexicon[word]\n",
    "            scores.append(score)\n",
    "            # Save ALL sentiment words\n",
    "            sentiment_words.append((word, score))\n",
    "    \n",
    "    # Calculate average\n",
    "    if not scores:\n",
    "        # No sentiment words found\n",
    "        return 0.0, {'total_words': len(words), 'sentiment_words': 0, 'top_positive': [], 'top_negative': []}\n",
    "    \n",
    "    avg_score = sum(scores) / len(words)  # Average over ALL words\n",
    "    \n",
    "    # Details for transparency\n",
    "    details = {\n",
    "        'total_words': len(words),\n",
    "        'sentiment_words': len(scores),\n",
    "        'top_positive': sorted([w for w in sentiment_words if w[1] > 0], \n",
    "                               key=lambda x: x[1], reverse=True)[:3],\n",
    "        'top_negative': sorted([w for w in sentiment_words if w[1] < 0], \n",
    "                               key=lambda x: x[1])[:3]\n",
    "    }\n",
    "    \n",
    "    return avg_score, details\n",
    "\n",
    "# Test with example texts\n",
    "test_texts = [\n",
    "    \"Das ist eine hervorragende und fantastische Lösung!\",\n",
    "    \"Die Situation ist katastrophal und inakzeptabel.\",\n",
    "    \"Der Antrag wurde zur Kenntnis genommen.\",\n",
    "    \"Der Winter ist zwar schön, aber auch ziemlich kalt.\"\n",
    "]\n",
    "\n",
    "if sentiws:\n",
    "    print(\"Sentiment function tests:\\n\")\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        score, details = analyze_sentiment_lexicon(text, sentiws)\n",
    "        print(f\"Text {i}: {text}\")\n",
    "        print(f\"  → Score: {score:+.4f}\")\n",
    "        print(f\"  → Positive words: {details['top_positive']}\")\n",
    "        print(f\"  → Negative words: {details['top_negative']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply to all speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentiws:\n",
    "    print(\"Analyzing sentiment with SentiWS lexicon...\\n\")\n",
    "    \n",
    "    results_lexicon = []\n",
    "    \n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing speeches\"):\n",
    "        score, details = analyze_sentiment_lexicon(\n",
    "            row['fliesstext'], \n",
    "            sentiws\n",
    "        )\n",
    "        results_lexicon.append({\n",
    "            'score': score,\n",
    "            'details': details\n",
    "        })\n",
    "    \n",
    "    df['sentiment_lexicon_score'] = [r['score'] for r in results_lexicon]\n",
    "    \n",
    "    print(\"\\n✓ Analysis completed!\")\n",
    "    print(f\"\\nAverage sentiment score: {df['sentiment_lexicon_score'].mean():+.4f}\")\n",
    "    print(f\"Score range: {df['sentiment_lexicon_score'].min():+.4f} to {df['sentiment_lexicon_score'].max():+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment by faction\n",
    "if sentiws:\n",
    "    sentiment_by_faction = df.groupby('fraktion')['sentiment_lexicon_score'].agg(['mean', 'count'])\n",
    "    sentiment_by_faction = sentiment_by_faction.sort_values('mean')\n",
    "    \n",
    "    print(\"\\nAverage sentiment by faction:\")\n",
    "    print(sentiment_by_faction)\n",
    "    \n",
    "    print(\"\\nObservations:\")\n",
    "    print(f\"- Most positive faction: {sentiment_by_faction.index[-1]} ({sentiment_by_faction['mean'].iloc[-1]:+.4f})\")\n",
    "    print(f\"- Most negative faction: {sentiment_by_faction.index[0]} ({sentiment_by_faction['mean'].iloc[0]:+.4f})\")\n",
    "    \n",
    "    # Most positive speech\n",
    "    most_positive_idx = df['sentiment_lexicon_score'].idxmax()\n",
    "    most_positive = df.loc[most_positive_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MOST POSITIVE SPEECH\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Speaker: {most_positive['redner_vorname']} {most_positive['redner_nachname']} ({most_positive['fraktion']})\")\n",
    "    print(f\"Score: {most_positive['sentiment_lexicon_score']:+.4f}\")\n",
    "    print(f\"\\nText (first 500 characters):\")\n",
    "    print(most_positive['fliesstext'][:500] + \"...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Most negative speech\n",
    "    most_negative_idx = df['sentiment_lexicon_score'].idxmin()\n",
    "    most_negative = df.loc[most_negative_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MOST NEGATIVE SPEECH\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Speaker: {most_negative['redner_vorname']} {most_negative['redner_nachname']} ({most_negative['fraktion']})\")\n",
    "    print(f\"Score: {most_negative['sentiment_lexicon_score']:+.4f}\")\n",
    "    print(f\"\\nText (first 500 characters):\")\n",
    "    print(most_negative['fliesstext'][:500] + \"...\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Lexicon-based Sentiment Analysis\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "**Advantages:**\n",
    "- Very transparent: You can see exactly which words contribute\n",
    "- Fast processing\n",
    "- Easy to understand and explain\n",
    "- Adjustable threshold\n",
    "\n",
    "**Limitations:**\n",
    "- No context understanding\n",
    "- \"nicht gut\" (not good) is recognized as positive (because \"gut\" is positive)\n",
    "- Irony is not recognized\n",
    "- Negations are not handled\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Model-based Approaches\n",
    "\n",
    "In this section, we use **pre-trained machine learning models** to analyze texts. These approaches are:\n",
    "- ✓ **Context-aware**: Understand negations, grammar, context\n",
    "- ✓ **Generalizable**: Can handle unknown words\n",
    "- ✓ **State-of-the-art**: Often better results\n",
    "- ✗ **Intransparent**: \"Black box\" - why this decision?\n",
    "- ✗ **Computationally intensive**: Slower, requires more resources\n",
    "\n",
    "We will implement a model-based approach for **NER**:\n",
    "- **NER**: Finding place names using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model-based NER with spaCy\n",
    "\n",
    "### How it works:\n",
    "\n",
    "**spaCy** is a popular NLP framework trained on millions of German texts. The model learned to recognize patterns:\n",
    "- Context (words around it)\n",
    "- Grammar (e.g., \"in Berlin\" → Berlin is probably a place)\n",
    "- Patterns learned from millions of examples\n",
    "\n",
    "The model can recognize:\n",
    "- **PER**: Persons\n",
    "- **LOC**: Locations (what we're interested in!)\n",
    "- **ORG**: Organizations\n",
    "- **MISC**: Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the German language model\n",
    "MODEL_NAME = 'de_core_news_sm'\n",
    "\n",
    "print(f\"Loading spaCy model '{MODEL_NAME}'...\")\n",
    "nlp = spacy.load(MODEL_NAME)\n",
    "print(\"✓ Model loaded!\")\n",
    "\n",
    "# What can the model recognize?\n",
    "print(\"\\nThe model recognizes the following entity types:\")\n",
    "print(\"- PER: Persons\")\n",
    "print(\"- LOC: Locations (what we're interested in!)\")\n",
    "print(\"- ORG: Organizations\")\n",
    "print(\"- MISC: Miscellaneous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_locations_spacy(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Finds locations in a text with spaCy.\n",
    "    \n",
    "    How it works:\n",
    "    1. The model analyzes the entire text\n",
    "    2. It recognizes \"entities\" (named entities) of various types\n",
    "    3. We filter out only the locations (LOC = Location)\n",
    "    \n",
    "    The model uses:\n",
    "    - Context (surrounding words)\n",
    "    - Grammar (e.g., \"in Berlin\" -> Berlin is probably a place)\n",
    "    - Patterns learned from millions of examples\n",
    "    \n",
    "    Parameters:\n",
    "        text: The text to analyze\n",
    "        nlp_model: The loaded spaCy model\n",
    "    \n",
    "    Return:\n",
    "        List of found locations\n",
    "    \"\"\"\n",
    "    # Run text through the model\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    # Extract only entities of type 'LOC' (Location)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == 'LOC']\n",
    "    \n",
    "    return locations\n",
    "\n",
    "# Test with the same example texts as before\n",
    "test_texts = [\n",
    "    \"In Berlin und München wird viel über die Politik in Bayern und Sachsen diskutiert.\",\n",
    "    \"Hamburg, Bremen und Kiel sind wichtige Hafenstädte.\",\n",
    "    \"Die Bundesregierung tagt in Berlin, die thüringische Landesregierung in Erfurt.\"\n",
    "]\n",
    "\n",
    "print(\"Function tests:\\n\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result_spacy = find_locations_spacy(text, nlp)\n",
    "    result_lexicon = find_locations_lexicon(text, locations_lower)\n",
    "    \n",
    "    print(f\"Test {i}: {text}\")\n",
    "    print(f\"  Found locations (spaCy):   {result_spacy}\")\n",
    "    print(f\"  Found locations (Lexicon): {result_lexicon}\")\n",
    "    print(f\"  Comparison: Does spaCy find the same locations as our lexicon?\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply to all speeches\n",
    "\n",
    "**Note**: This can take a few minutes as the model analyzes each text individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing speeches with spaCy model...\")\n",
    "print(\"(This can take 2-5 minutes)\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['ner_spacy'] = df['fliesstext'].progress_apply(\n",
    "    lambda text: find_locations_spacy(text, nlp)\n",
    ")\n",
    "\n",
    "df['ner_spacy_count'] = df['ner_spacy'].apply(len)\n",
    "\n",
    "print(f\"\\n✓ Analysis completed!\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"- Speeches with at least one location: {(df['ner_spacy_count'] > 0).sum()} of {len(df)}\")\n",
    "print(f\"- Average {df['ner_spacy_count'].mean():.2f} locations per speech\")\n",
    "print(f\"- Maximum: {df['ner_spacy_count'].max()} locations in one speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which locations does spaCy find most frequently?\n",
    "all_locations_spacy = [loc for locs in df['ner_spacy'] for loc in locs]\n",
    "location_counts_spacy = Counter(all_locations_spacy)\n",
    "\n",
    "print(\"Top 15 most mentioned locations (spaCy method):\")\n",
    "for location, count in location_counts_spacy.most_common(15):\n",
    "    print(f\"  {location}: {count}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare Lexicon vs. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which locations does spaCy find that the lexicon doesn't?\n",
    "spacy_unique = set(all_locations_spacy) - set(all_locations_lexicon)\n",
    "lexicon_unique = set(all_locations_lexicon) - set(all_locations_spacy)\n",
    "both = set(all_locations_spacy) & set(all_locations_lexicon)\n",
    "\n",
    "print(\"Differences in found locations:\\n\")\n",
    "print(f\"Only found by spaCy: {len(spacy_unique)} locations\")\n",
    "print(f\"Examples: {list(spacy_unique)[:10]}\\n\")\n",
    "\n",
    "print(f\"Only found by Lexicon: {len(lexicon_unique)} locations\")\n",
    "print(f\"Examples: {list(lexicon_unique)[:10]}\\n\")\n",
    "\n",
    "print(f\"Found by both: {len(both)} locations\")\n",
    "print(f\"Examples: {list(both)[:10]}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"- Lexicon finds on average {df['ner_lexicon_count'].mean():.2f} locations per speech\")\n",
    "print(f\"- spaCy finds on average {df['ner_spacy_count'].mean():.2f} locations per speech\")\n",
    "print(f\"- Difference: {abs(df['ner_lexicon_count'].mean() - df['ner_spacy_count'].mean()):.2f} locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis: Look at an example speech\n",
    "# ADJUSTMENT: Change the number to analyze a different speech\n",
    "EXAMPLE_SPEECH_INDEX = 0\n",
    "\n",
    "example = df.iloc[EXAMPLE_SPEECH_INDEX]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"EXAMPLE SPEECH #{EXAMPLE_SPEECH_INDEX}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Speaker: {example['redner_vorname']} {example['redner_nachname']} ({example['fraktion']})\")\n",
    "print(f\"\\nText (first 500 characters):\")\n",
    "print(example['fliesstext'][:500] + \"...\")\n",
    "print(f\"\\n{'─' * 80}\")\n",
    "print(f\"Found locations (Lexicon): {example['ner_lexicon']}\")\n",
    "print(f\"Found locations (spaCy):   {example['ner_spacy']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Model-based vs. Lexicon-based NER\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Quantity**: Which method finds more locations? Why?\n",
    "\n",
    "2. **Quality**: \n",
    "   - Does spaCy also find locations not in our lexicon?\n",
    "   - Does spaCy make errors (False Positives = recognizes something as a location that isn't)?\n",
    "   \n",
    "3. **Overlap**: How large is the intersection? Do both methods often agree?\n",
    "\n",
    "**Typical differences:**\n",
    "- Lexicon: Only finds known locations, no errors with correct list\n",
    "- Model: Also finds unknown locations, but can also make errors\n",
    "\n",
    "**Advantages of the model:**\n",
    "- Can recognize unknown locations\n",
    "- Understands context\n",
    "- Can handle variations\n",
    "\n",
    "**Disadvantages of the model:**\n",
    "- Less transparent\n",
    "- Can make mistakes\n",
    "- Computationally intensive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
