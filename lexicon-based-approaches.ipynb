{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of EU Parliament Speeches: Comparing Lexicon-based and Model-based Approaches\n",
    "\n",
    "In this notebook, we analyze speeches from the European Parliament and compare two fundamentally different approaches to text analysis:\n",
    "\n",
    "1. **Lexicon-based approaches**: Work with predefined dictionaries and rules\n",
    "2. **Model-based approaches**: Use machine learning and pre-trained AI models\n",
    "\n",
    "We will apply both approaches to two tasks:\n",
    "- **Named Entity Recognition (NER)**: Finding location names in texts\n",
    "- **Sentiment Analysis**: Determining the emotional tone (positive/negative/neutral)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Understand the differences between rule-based and ML-based approaches\n",
    "- Apply both methods practically\n",
    "- Assess the advantages and disadvantages of both approaches\n",
    "- Compare and interpret results\n",
    "\n",
    "## Structure\n",
    "\n",
    "1. Setup and Data Loading\n",
    "2. **Lexicon-based Approaches**\n",
    "   - Named Entity Recognition with location dictionary\n",
    "   - Sentiment Analysis with VADER lexicon\n",
    "3. **Model-based Approaches**\n",
    "   - Named Entity Recognition with spaCy\n",
    "4. Comparison and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Installation\n",
    "\n",
    "First, we will install all required packages. You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install packages (only needed the first time)\n",
    "!pip install tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We load the EU Parliament speeches from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches: 1828\n",
      "\n",
      "Columns: ['speaker', 'text', 'party', 'date', 'agenda', 'speechnumber', 'procedure_ID', 'partyfacts_ID', 'period', 'chair', 'MEP', 'commission', 'written', 'multispeaker', 'link', 'translationInSpeech', 'translatedText']\n",
      "\n",
      "First rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>party</th>\n",
       "      <th>date</th>\n",
       "      <th>agenda</th>\n",
       "      <th>speechnumber</th>\n",
       "      <th>procedure_ID</th>\n",
       "      <th>partyfacts_ID</th>\n",
       "      <th>period</th>\n",
       "      <th>chair</th>\n",
       "      <th>MEP</th>\n",
       "      <th>commission</th>\n",
       "      <th>written</th>\n",
       "      <th>multispeaker</th>\n",
       "      <th>link</th>\n",
       "      <th>translationInSpeech</th>\n",
       "      <th>translatedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President</td>\n",
       "      <td>I wish you an excellent good morning on this l...</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>2. Interinstitutional Body for Ethical Standar...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.europarl.europa.eu/doceo/document/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President</td>\n",
       "      <td>Dear colleagues, today marks the 50th annivers...</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>6. Statements by the President</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.europarl.europa.eu/doceo/document/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virginijus Sinkevičius</td>\n",
       "      <td>Mr President, honourable Members, dear rapport...</td>\n",
       "      <td>-</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>4. Framework of measures for strengthening Eur...</td>\n",
       "      <td>7</td>\n",
       "      <td>bill_165_ID bill_195_ID  bill_165_ID bill_195_ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.europarl.europa.eu/doceo/document/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anna Deparnay-Grunenberg</td>\n",
       "      <td>Mr President, I came to this House to be a voi...</td>\n",
       "      <td>Greens/EFA</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>4. Framework of measures for strengthening Eur...</td>\n",
       "      <td>9</td>\n",
       "      <td>bill_165_ID bill_195_ID  bill_165_ID bill_195_ID</td>\n",
       "      <td>6403.0</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.europarl.europa.eu/doceo/document/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seán Kelly</td>\n",
       "      <td>A Uachtaráin, teastaíonn uaim mo thacaíocht io...</td>\n",
       "      <td>PPE</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>4. Framework of measures for strengthening Eur...</td>\n",
       "      <td>24</td>\n",
       "      <td>bill_165_ID bill_195_ID  bill_165_ID bill_195_ID</td>\n",
       "      <td>6398.0</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.europarl.europa.eu/doceo/document/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    speaker  \\\n",
       "0                 President   \n",
       "1                 President   \n",
       "2    Virginijus Sinkevičius   \n",
       "3  Anna Deparnay-Grunenberg   \n",
       "4                Seán Kelly   \n",
       "\n",
       "                                                text       party        date  \\\n",
       "0  I wish you an excellent good morning on this l...           -  2024-04-25   \n",
       "1  Dear colleagues, today marks the 50th annivers...           -  2024-04-25   \n",
       "2  Mr President, honourable Members, dear rapport...           -  2024-04-25   \n",
       "3  Mr President, I came to this House to be a voi...  Greens/EFA  2024-04-25   \n",
       "4  A Uachtaráin, teastaíonn uaim mo thacaíocht io...         PPE  2024-04-25   \n",
       "\n",
       "                                              agenda  speechnumber  \\\n",
       "0  2. Interinstitutional Body for Ethical Standar...             1   \n",
       "1                     6. Statements by the President             1   \n",
       "2  4. Framework of measures for strengthening Eur...             7   \n",
       "3  4. Framework of measures for strengthening Eur...             9   \n",
       "4  4. Framework of measures for strengthening Eur...            24   \n",
       "\n",
       "                                        procedure_ID  partyfacts_ID  period  \\\n",
       "0                                                               NaN       9   \n",
       "1                                                               NaN       9   \n",
       "2   bill_165_ID bill_195_ID  bill_165_ID bill_195_ID            NaN       9   \n",
       "3   bill_165_ID bill_195_ID  bill_165_ID bill_195_ID         6403.0       9   \n",
       "4   bill_165_ID bill_195_ID  bill_165_ID bill_195_ID         6398.0       9   \n",
       "\n",
       "   chair    MEP  commission  written  multispeaker  \\\n",
       "0   True  False       False    False         False   \n",
       "1   True  False       False    False         False   \n",
       "2  False  False        True    False         False   \n",
       "3  False   True       False    False         False   \n",
       "4  False   True       False    False         False   \n",
       "\n",
       "                                                link  translationInSpeech  \\\n",
       "0  https://www.europarl.europa.eu/doceo/document/...                  NaN   \n",
       "1  https://www.europarl.europa.eu/doceo/document/...                  NaN   \n",
       "2  https://www.europarl.europa.eu/doceo/document/...                  NaN   \n",
       "3  https://www.europarl.europa.eu/doceo/document/...                  NaN   \n",
       "4  https://www.europarl.europa.eu/doceo/document/...                  NaN   \n",
       "\n",
       "   translatedText  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv('eu_speeches_2024_english.csv')\n",
    "\n",
    "print(f\"Number of speeches: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution by party:\n",
      "party\n",
      "-             665\n",
      "PPE           290\n",
      "The Left      223\n",
      "Renew         202\n",
      "Greens/EFA    193\n",
      "S&D           152\n",
      "ECR            68\n",
      "NI             26\n",
      "ID              9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range: 2024-01-15 to 2024-04-25\n",
      "Average length of a speech: 1530 characters\n"
     ]
    }
   ],
   "source": [
    "# Brief overview of the data\n",
    "print(\"Distribution by party:\")\n",
    "print(df['party'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Average length of a speech: {df['text'].str.len().mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Lexicon-based Approaches\n",
    "\n",
    "In this section, we use **predefined dictionaries and word lists** to analyze texts. These approaches are:\n",
    "- ✓ **Transparent**: Every decision is traceable\n",
    "- ✓ **Fast**: Very quick processing\n",
    "- ✓ **Controllable**: You can modify the lexicons\n",
    "- ✗ **Limited**: Only finds what's in the dictionary\n",
    "- ✗ **No context understanding**: Cannot understand negations or irony\n",
    "\n",
    "We will implement two lexicon-based approaches:\n",
    "1. **NER**: Finding place names using a location dictionary\n",
    "2. **Sentiment Analysis**: Analyzing emotional tone using the VADER lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lexicon-based NER: Finding Place Names\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. We create a list of European locations (cities, countries, regions)\n",
    "2. We split each text into words\n",
    "3. For each word, we check: Is it in our location dictionary?\n",
    "4. If yes, we mark it as a location\n",
    "\n",
    "This is the **simplest form of Named Entity Recognition**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the location lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location lexicon created with 105 entries\n",
      "\n",
      "Examples: ['Yorkshire', 'Leeds', 'Turin', 'Malaga', 'Rome', 'Austria', 'Lithuania', 'Copenhagen', 'Norway', 'France']\n"
     ]
    }
   ],
   "source": [
    "# Base lexicon: European cities, countries, and regions\n",
    "# ADJUSTMENT: You can add or remove locations here!\n",
    "\n",
    "european_locations = {\n",
    "    # Major capitals\n",
    "    'Paris', 'Berlin', 'Rome', 'Madrid', 'Warsaw', 'Brussels',\n",
    "    'Vienna', 'Athens', 'Lisbon', 'Budapest', 'Prague', 'Stockholm',\n",
    "    'Amsterdam', 'Copenhagen', 'Dublin', 'Helsinki', 'Bucharest',\n",
    "    'Sofia', 'Zagreb', 'Ljubljana', 'Bratislava', 'Tallinn', 'Riga',\n",
    "    'Vilnius', 'Valletta', 'Luxembourg', 'Nicosia',\n",
    "    \n",
    "    # Other major cities\n",
    "    'Barcelona', 'Milan', 'Munich', 'Hamburg', 'Lyon', 'Marseille',\n",
    "    'Krakow', 'Gdansk', 'Porto', 'Valencia', 'Turin', 'Naples',\n",
    "    'Rotterdam', 'Frankfurt', 'Cologne', 'Stuttgart', 'Seville',\n",
    "    'Bilbao', 'Malaga', 'Manchester', 'Birmingham', 'Leeds', 'Liverpool',\n",
    "    'Glasgow', 'Edinburgh', 'Belfast', 'Cardiff', 'Bristol',\n",
    "    \n",
    "    # EU Countries\n",
    "    'Germany', 'France', 'Italy', 'Spain', 'Poland', 'Romania',\n",
    "    'Netherlands', 'Belgium', 'Greece', 'Portugal', 'Sweden',\n",
    "    'Hungary', 'Austria', 'Bulgaria', 'Denmark', 'Finland',\n",
    "    'Slovakia', 'Ireland', 'Croatia', 'Slovenia', 'Lithuania',\n",
    "    'Latvia', 'Estonia', 'Cyprus', 'Malta', 'Luxembourg',\n",
    "    \n",
    "    # Regions\n",
    "    'Catalonia', 'Bavaria', 'Andalusia', 'Tuscany', 'Brittany',\n",
    "    'Flanders', 'Scotland', 'Wales', 'Corsica', 'Sicily',\n",
    "    'Lombardy', 'Yorkshire', 'Lancashire', 'Saxony', 'Brandenburg',\n",
    "    \n",
    "    # Non-EU but commonly mentioned\n",
    "    'London', 'UK', 'United Kingdom', 'Britain', 'England',\n",
    "    'Ukraine', 'Russia', 'Switzerland', 'Norway', 'Turkey'\n",
    "}\n",
    "\n",
    "# For searching: both original and lowercase\n",
    "locations_lower = {loc.lower(): loc for loc in european_locations}\n",
    "\n",
    "print(f\"Location lexicon created with {len(european_locations)} entries\")\n",
    "print(f\"\\nExamples: {list(european_locations)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement the search function\n",
    "\n",
    "Now we program the function that searches for locations in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function tests:\n",
      "\n",
      "Test 1:\n",
      "  Text: In Paris and Berlin, we are discussing the future of Europe with partners from Madrid and Rome.\n",
      "  Found locations: ['Paris', 'Berlin', 'Madrid', 'Rome']\n",
      "  Count: 4\n",
      "\n",
      "Test 2:\n",
      "  Text: Poland, Hungary, and the Czech Republic have different views than France and Germany.\n",
      "  Found locations: ['Poland', 'Hungary', 'France', 'Germany']\n",
      "  Count: 4\n",
      "\n",
      "Test 3:\n",
      "  Text: The crisis affects Greece, Italy, Spain, and Portugal, but also Ireland and Cyprus.\n",
      "  Found locations: ['Greece', 'Italy', 'Spain', 'Portugal', 'Ireland', 'Cyprus']\n",
      "  Count: 6\n",
      "\n",
      "Observation:\n",
      "The function only finds locations that\n",
      "  1. Are in the lexicon AND\n",
      "  2. Appear exactly like that in the text\n",
      "\n",
      "Experiment: Add a new location to the lexicon above\n",
      "   and run both cells again. Will it be found now?\n"
     ]
    }
   ],
   "source": [
    "def find_locations_lexicon(text, location_dict):\n",
    "    \"\"\"\n",
    "    Finds locations in a text based on a lexicon.\n",
    "    \n",
    "    How it works (very simple):\n",
    "    1. Split the text into individual words (at spaces)\n",
    "    2. For each word: Check if it's in our location lexicon\n",
    "    3. If yes: Save it\n",
    "    \n",
    "    This is the simplest form of Named Entity Recognition!\n",
    "    \n",
    "    Parameters:\n",
    "        text: The text to analyze\n",
    "        location_dict: Dictionary with locations (lowercase -> Original)\n",
    "    \n",
    "    Return:\n",
    "        List of found locations\n",
    "    \"\"\"\n",
    "    found_locations = []\n",
    "    \n",
    "    # Split text into words (at spaces)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Go through each word\n",
    "    for word in words:\n",
    "        # Remove punctuation (e.g., \"Paris,\" -> \"Paris\")\n",
    "        word_clean = word.strip('.,;:!?()[]\"\\'')\n",
    "        \n",
    "        # Check if the word (in lowercase) is in our lexicon\n",
    "        if word_clean.lower() in location_dict:\n",
    "            # Add the original spelling\n",
    "            found_locations.append(location_dict[word_clean.lower()])\n",
    "    \n",
    "    return found_locations\n",
    "\n",
    "# Test with example texts\n",
    "test_texts = [\n",
    "    \"In Paris and Berlin, we are discussing the future of Europe with partners from Madrid and Rome.\",\n",
    "    \"Poland, Hungary, and the Czech Republic have different views than France and Germany.\",\n",
    "    \"The crisis affects Greece, Italy, Spain, and Portugal, but also Ireland and Cyprus.\"\n",
    "]\n",
    "\n",
    "print(\"Function tests:\\n\")\n",
    "for i, test_text in enumerate(test_texts, 1):\n",
    "    test_result = find_locations_lexicon(test_text, locations_lower)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"  Text: {test_text}\")\n",
    "    print(f\"  Found locations: {test_result}\")\n",
    "    print(f\"  Count: {len(test_result)}\\n\")\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"The function only finds locations that\")\n",
    "print(\"  1. Are in the lexicon AND\")\n",
    "print(\"  2. Appear exactly like that in the text\")\n",
    "print(\"\\nExperiment: Add a new location to the lexicon above\")\n",
    "print(\"   and run both cells again. Will it be found now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply to all speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing speeches with lexicon-based NER...\n",
      "✓ Analysis completed!\n",
      "\n",
      "Statistics:\n",
      "- Speeches with at least one location: 693 of 1828\n",
      "- Average 1.21 locations per speech\n",
      "- Maximum: 33 locations in one speech\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to all speeches\n",
    "print(\"Analyzing speeches with lexicon-based NER...\")\n",
    "\n",
    "df['ner_lexicon'] = df['text'].apply(\n",
    "    lambda text: find_locations_lexicon(text, locations_lower)\n",
    ")\n",
    "\n",
    "# Number of found locations per speech\n",
    "df['ner_lexicon_count'] = df['ner_lexicon'].apply(len)\n",
    "\n",
    "print(f\"✓ Analysis completed!\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"- Speeches with at least one location: {(df['ner_lexicon_count'] > 0).sum()} of {len(df)}\")\n",
    "print(f\"- Average {df['ner_lexicon_count'].mean():.2f} locations per speech\")\n",
    "print(f\"- Maximum: {df['ner_lexicon_count'].max()} locations in one speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most mentioned locations (lexicon method):\n",
      "  Ukraine: 747x\n",
      "  Russia: 394x\n",
      "  Ireland: 169x\n",
      "  Hungary: 99x\n",
      "  Finland: 58x\n",
      "  Germany: 55x\n",
      "  Greece: 54x\n",
      "  Brussels: 53x\n",
      "  Romania: 52x\n",
      "  Poland: 39x\n",
      "  Spain: 37x\n",
      "  France: 37x\n",
      "  Italy: 32x\n",
      "  Sweden: 29x\n",
      "  Slovakia: 28x\n"
     ]
    }
   ],
   "source": [
    "# Which locations are mentioned most frequently?\n",
    "all_locations_lexicon = [loc for locs in df['ner_lexicon'] for loc in locs]\n",
    "location_counts_lexicon = Counter(all_locations_lexicon)\n",
    "\n",
    "print(\"Top 15 most mentioned locations (lexicon method):\")\n",
    "for location, count in location_counts_lexicon.most_common(15):\n",
    "    print(f\"  {location}: {count}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Lexicon-based NER\n",
    "\n",
    "**What did we observe?**\n",
    "- The method only finds locations that are in our lexicon\n",
    "- The results are 100% traceable\n",
    "- Smaller cities or regions are not found (unless we add them)\n",
    "\n",
    "**Advantages:**\n",
    "- Very transparent and understandable\n",
    "- Fast processing\n",
    "- No false positives (if the lexicon is correct)\n",
    "- Easy to customize\n",
    "\n",
    "**Disadvantages:**\n",
    "- Only finds known locations\n",
    "- Cannot handle variations\n",
    "- Requires manual maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Lexicon-based Sentiment Analysis with VADER\n",
    "\n",
    "### How it works:\n",
    "\n",
    "**VADER** (Valence Aware Dictionary and sEntiment Reasoner) is an English sentiment analysis tool specifically designed for social media and political text. It contains approximately 7,500 words with sentiment scores.\n",
    "\n",
    "**The algorithm:**\n",
    "1. Split text into words\n",
    "2. For each word: Look up the score in the lexicon\n",
    "   - Positive words have scores > 0 (e.g., \"good\" = +0.7)\n",
    "   - Negative words have scores < 0 (e.g., \"bad\" = -0.7)\n",
    "3. Apply rules for:\n",
    "   - Punctuation (\"!!!\" increases intensity)\n",
    "   - Capitalization (\"GREAT\" is stronger than \"great\")\n",
    "   - Negations (\"not good\" is negative)\n",
    "4. Calculate compound score (-1 to +1)\n",
    "\n",
    "### Why VADER?\n",
    "\n",
    "- Specifically designed for political and social media text\n",
    "- Handles negations better than simple lexicons\n",
    "- Understands intensifiers (\"very good\", \"extremely bad\")\n",
    "- Works well for EU Parliament debate style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ VADER lexicon successfully loaded!\n",
      "Lexicon contains 7494 words\n",
      "\n",
      "Examples of positive words:\n",
      "  freewheel: +0.500\n",
      "  vitalities: +1.200\n",
      "  beneficial: +1.900\n",
      "  hhok: +0.900\n",
      "  (':: +2.300\n",
      "\n",
      "Examples of negative words:\n",
      "  stupids: -2.300\n",
      "  bribe: -0.800\n",
      "  coward: -2.000\n",
      "  v.v: -2.900\n",
      "  inferiority: -1.100\n",
      "\n",
      "Statistics:\n",
      "  Positive words: 3329\n",
      "  Negative words: 4165\n",
      "  Total: 7494\n"
     ]
    }
   ],
   "source": [
    "def load_vader_lexicon(lexicon_file):\n",
    "    \"\"\"\n",
    "    Loads the VADER lexicon from a text file.\n",
    "    \n",
    "    File format:\n",
    "    word    score\n",
    "    e.g.: good    1.9\n",
    "    \n",
    "    Return:\n",
    "        Dictionary: {word: score}\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    \n",
    "    try:\n",
    "        with open(lexicon_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Skip comments and empty lines\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                \n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    word = parts[0]\n",
    "                    score = float(parts[1])\n",
    "                    lexicon[word.lower()] = score\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {lexicon_file}\")\n",
    "        print(\"Please run the extraction script first to create vader_lexicon.txt\")\n",
    "        return None\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "# Load VADER lexicon\n",
    "vader_lex = load_vader_lexicon('vader_lexicon.txt')\n",
    "\n",
    "if vader_lex:\n",
    "    print(f\"✓ VADER lexicon successfully loaded!\")\n",
    "    print(f\"Lexicon contains {len(vader_lex)} words\\n\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"Examples of positive words:\")\n",
    "    positive_words = {k: v for k, v in vader_lex.items() if v > 0}\n",
    "    positive_items = list(positive_words.items())\n",
    "    random.shuffle(positive_items)\n",
    "    for word, score in positive_items[:5]:\n",
    "        print(f\"  {word}: {score:+.3f}\")\n",
    "    \n",
    "    print(\"\\nExamples of negative words:\")\n",
    "    negative_words = {k: v for k, v in vader_lex.items() if v < 0}\n",
    "    negative_items = list(negative_words.items())\n",
    "    random.shuffle(negative_items)\n",
    "    for word, score in negative_items[:5]:\n",
    "        print(f\"  {word}: {score:+.3f}\")\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Positive words: {len(positive_words)}\")\n",
    "    print(f\"  Negative words: {len(negative_words)}\")\n",
    "    print(f\"  Total: {len(vader_lex)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement sentiment analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_lexicon(text, lexicon):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of a text using a lexicon.\n",
    "    \n",
    "    Simplified version - just averages word scores.\n",
    "    (VADER library also handles punctuation, capitalization, etc.)\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Split text into words\n",
    "    2. For each word: Look up the score in the lexicon\n",
    "    3. Calculate average of all scores\n",
    "    \n",
    "    Parameters:\n",
    "        text: Text to analyze\n",
    "        lexicon: VADER dictionary\n",
    "    \n",
    "    Return:\n",
    "        (score, details)\n",
    "    \"\"\"\n",
    "    # Split text into words (only letters)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "    \n",
    "    # Collect scores\n",
    "    scores = []\n",
    "    sentiment_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in lexicon:\n",
    "            score = lexicon[word]\n",
    "            scores.append(score)\n",
    "            sentiment_words.append((word, score))\n",
    "    \n",
    "    # Calculate average\n",
    "    if not scores:\n",
    "        return 0.0, {\n",
    "            'total_words': len(words),\n",
    "            'sentiment_words': 0,\n",
    "            'top_positive': [],\n",
    "            'top_negative': []\n",
    "        }\n",
    "    \n",
    "    # Normalize to -1 to +1 range (VADER scores are -4 to +4)\n",
    "    avg_score = sum(scores) / len(words)\n",
    "    normalized_score = avg_score / 4.0  # Normalize to -1 to +1\n",
    "    \n",
    "    details = {\n",
    "        'total_words': len(words),\n",
    "        'sentiment_words': len(scores),\n",
    "        'top_positive': sorted([w for w in sentiment_words if w[1] > 0],\n",
    "                               key=lambda x: x[1], reverse=True)[:3],\n",
    "        'top_negative': sorted([w for w in sentiment_words if w[1] < 0],\n",
    "                               key=lambda x: x[1])[:3]\n",
    "    }\n",
    "    \n",
    "    return normalized_score, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply to all speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment with VADER...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1828/1828 [00:00<00:00, 6899.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Analysis completed!\n",
      "\n",
      "Average sentiment score: +0.0076\n",
      "Score range: -0.0716 to +0.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing sentiment with VADER...\\n\")\n",
    "\n",
    "results_vader = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing speeches\"):\n",
    "    score, details = analyze_sentiment_lexicon(row['text'], vader_lex)\n",
    "    results_vader.append({\n",
    "        'score': score,\n",
    "        'details': details\n",
    "    })\n",
    "\n",
    "df['sentiment_vader_score'] = [r['score'] for r in results_vader]\n",
    "\n",
    "print(\"\\n✓ Analysis completed!\")\n",
    "print(f\"\\nAverage sentiment score: {df['sentiment_vader_score'].mean():+.4f}\")\n",
    "print(f\"Score range: {df['sentiment_vader_score'].min():+.4f} to {df['sentiment_vader_score'].max():+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average sentiment by party:\n",
      "                mean  count\n",
      "party                      \n",
      "The Left   -0.004680    223\n",
      "NI          0.002666     26\n",
      "ID          0.002724      9\n",
      "Greens/EFA  0.005032    193\n",
      "ECR         0.005845     68\n",
      "Renew       0.006121    202\n",
      "S&D         0.006675    152\n",
      "PPE         0.009348    290\n",
      "-           0.012810    665\n",
      "\n",
      "Observations:\n",
      "- Most positive party: - (+0.0128)\n",
      "- Most negative party: The Left (-0.0047)\n",
      "\n",
      "================================================================================\n",
      "MOST POSITIVE SPEECH\n",
      "================================================================================\n",
      "Speaker: Reinhard Bütikofer (Greens/EFA)\n",
      "Date: 2024-02-06\n",
      "Score: +0.0866\n",
      "\n",
      "Text (first 500 characters):\n",
      "Colleague, as you just expressed your strong support for helping Ukraine to help us defend the European security architecture and our freedom, would you support the Estonian proposal that every EU Member State should dedicate 0.25 % of their GDP to military support for Ukraine?...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MOST NEGATIVE SPEECH\n",
      "================================================================================\n",
      "Speaker: President (-)\n",
      "Date: 2024-03-13\n",
      "Score: -0.0716\n",
      "\n",
      "Text (first 500 characters):\n",
      "The next item is the debate on the Commission statement on rising anti-LGBTIQ rhetoric and violence: recent attacks in Thessaloniki (2024/2654(RSP)....\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sentiment by party group\n",
    "sentiment_by_party = df.groupby('party')['sentiment_vader_score'].agg(['mean', 'count'])\n",
    "sentiment_by_party = sentiment_by_party.sort_values('mean')\n",
    "\n",
    "print(\"\\nAverage sentiment by party:\")\n",
    "print(sentiment_by_party)\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(f\"- Most positive party: {sentiment_by_party.index[-1]} ({sentiment_by_party['mean'].iloc[-1]:+.4f})\")\n",
    "print(f\"- Most negative party: {sentiment_by_party.index[0]} ({sentiment_by_party['mean'].iloc[0]:+.4f})\")\n",
    "\n",
    "# Most positive speech\n",
    "most_positive_idx = df['sentiment_vader_score'].idxmax()\n",
    "most_positive = df.loc[most_positive_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MOST POSITIVE SPEECH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Speaker: {most_positive['speaker']} ({most_positive['party']})\")\n",
    "print(f\"Date: {most_positive['date']}\")\n",
    "print(f\"Score: {most_positive['sentiment_vader_score']:+.4f}\")\n",
    "print(f\"\\nText (first 500 characters):\")\n",
    "print(most_positive['text'][:500] + \"...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Most negative speech\n",
    "most_negative_idx = df['sentiment_vader_score'].idxmin()\n",
    "most_negative = df.loc[most_negative_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MOST NEGATIVE SPEECH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Speaker: {most_negative['speaker']} ({most_negative['party']})\")\n",
    "print(f\"Date: {most_negative['date']}\")\n",
    "print(f\"Score: {most_negative['sentiment_vader_score']:+.4f}\")\n",
    "print(f\"\\nText (first 500 characters):\")\n",
    "print(most_negative['text'][:500] + \"...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Lexicon-based Sentiment Analysis\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "**Advantages:**\n",
    "- Transparent: VADER shows positive/negative/neutral proportions\n",
    "- Fast processing\n",
    "- Handles negations (\"not good\" = negative)\n",
    "- Understands intensifiers (\"very\", \"extremely\")\n",
    "- Designed for social and political text\n",
    "\n",
    "**Limitations:**\n",
    "- Still limited context understanding\n",
    "- May miss subtle political rhetoric\n",
    "- Complex irony not always detected\n",
    "- Domain-specific political terminology may not be in lexicon\n",
    "   \n",
    "→ That's why models can be even better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
